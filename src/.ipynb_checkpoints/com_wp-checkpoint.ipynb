{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "score_dict=defaultdict(int)\n",
    "float('18.4243')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(2.1605745193344417, '../models/abs_bert/model_step_148000.pt'),\n",
    " (2.1630803167590136, '../models/abs_bert/model_step_140000.pt'),\n",
    " (2.1644197772150187, '../models/abs_bert/model_step_136000.pt'),\n",
    " (2.165481045323107, '../models/abs_bert/model_step_166000.pt'), \n",
    " (2.1655918786872848, '../models/abs_bert/model_step_122000.pt')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(2.1655918786872848)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'step:2000': 44.4382,\n",
       "             'step:12000': 14.8503,\n",
       "             'step:8000': 17.4382,\n",
       "             'step:10000': 16.3751,\n",
       "             'step:14000': 13.5733,\n",
       "             'step:16000': 12.8653,\n",
       "             'step:18000': 12.4152,\n",
       "             'step:100000': 8.98825,\n",
       "             'step:4000': 22.3048,\n",
       "             'step:6000': 18.8529,\n",
       "             'step:20000': 11.7929,\n",
       "             'step:22000': 11.4602,\n",
       "             'step:24000': 10.9593,\n",
       "             'step:26000': 10.6905,\n",
       "             'step:28000': 10.6443,\n",
       "             'step:30000': 10.5071,\n",
       "             'step:32000': 10.2203,\n",
       "             'step:34000': 10.1402,\n",
       "             'step:36000': 9.97889,\n",
       "             'step:38000': 9.88723,\n",
       "             'step:40000': 9.70943,\n",
       "             'step:42000': 9.66159,\n",
       "             'step:44000': 9.53665,\n",
       "             'step:46000': 9.5887,\n",
       "             'step:48000': 9.55114,\n",
       "             'step:50000': 9.35607,\n",
       "             'step:52000': 9.39798,\n",
       "             'step:54000': 9.31268,\n",
       "             'step:56000': 9.20929,\n",
       "             'step:58000': 9.21339,\n",
       "             'step:60000': 9.13269,\n",
       "             'step:62000': 9.10735,\n",
       "             'step:64000': 9.13195,\n",
       "             'step:66000': 9.03086,\n",
       "             'step:68000': 8.97046,\n",
       "             'step:70000': 9.0399,\n",
       "             'step:72000': 9.0501,\n",
       "             'step:74000': 8.95597,\n",
       "             'step:76000': 8.922,\n",
       "             'step:78000': 8.91978,\n",
       "             'step:80000': 8.93225,\n",
       "             'step:82000': 8.9642,\n",
       "             'step:102000': 0,\n",
       "             'step:84000': 8.85713,\n",
       "             'step:86000': 8.83719,\n",
       "             'step:88000': 8.89783,\n",
       "             'step:90000': 8.88131,\n",
       "             'step:92000': 8.84566,\n",
       "             'step:94000': 8.82508,\n",
       "             'step:96000': 8.83439,\n",
       "             'step:98000': 8.88952,\n",
       "             'step:104000': 8.76137,\n",
       "             'step:106000': 8.82774,\n",
       "             'step:108000': 8.82185,\n",
       "             'step:110000': 8.79892,\n",
       "             'step:112000': 8.80969,\n",
       "             'step:114000': 0,\n",
       "             'step:116000': 8.9215,\n",
       "             'step:118000': 8.76684,\n",
       "             'step:120000': 8.75476,\n",
       "             'step:122000': 0,\n",
       "             'step:124000': 8.7658,\n",
       "             'step:126000': 8.77025,\n",
       "             'step:128000': 0,\n",
       "             'step:130000': 8.78066,\n",
       "             'step:132000': 8.81388,\n",
       "             'step:134000': 8.79102,\n",
       "             'step:136000': 8.74534,\n",
       "             'step:138000': 8.77777,\n",
       "             'step:140000': 0,\n",
       "             'step:142000': 8.76556,\n",
       "             'step:144000': 8.80894})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open('../logs/val_abs_bert_cnndm', 'r') as f:\n",
    "    cur_step=None\n",
    "    for line in f.readlines():\n",
    "        if 'Loading checkpoint from' in line:\n",
    "            cur_step='step:'+line.split('model_step_')[1].replace('.pt\\n','')\n",
    "            score_dict[cur_step]=0\n",
    "        if 'Validation perplexity:' in line:\n",
    "\n",
    "            score_dict[cur_step]=float(line.split('perplexity: ')[1].replace('\\n', ''))\n",
    "score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "import torch\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from pytorch_transformers import BertTokenizer\n",
    "\n",
    "import distributed\n",
    "# from models import data_loader, model_builder\n",
    "\n",
    "from models.loss import abs_loss\n",
    "from models.model_builder import AbsSummarizer\n",
    "from models.predictor import build_predictor\n",
    "from models.trainer import build_trainer\n",
    "from others.logging import logger, init_logger\n",
    "checkpoint = torch.load('/data/bqw/GraphSum_com/models/abs_bert/model_step_8000.pt', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n",
      "True\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3]], which is output 0 of SigmoidBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-9dbf367b3743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3]], which is output 0 of SigmoidBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3.], requires_grad = True)\n",
    "out = a.sigmoid()\n",
    "c = out.detach()\n",
    "print(c.grad_fn)\n",
    "c.zero_()  \n",
    "\n",
    "out  # modified by c.zero_() !!\n",
    "print(c.requires_grad)\n",
    "print(out.requires_grad)\n",
    "out.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3.], requires_grad = True)\n",
    "out = a.sigmoid()\n",
    "c = out.data\n",
    "c.zero_()  \n",
    "\n",
    "out  # modified by c.zero_() !!\n",
    "\n",
    "out.sum().backward()\n",
    "print(c.requires_grad)\n",
    "print(out.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint['model'].keys()\n",
    "a=None\n",
    "a is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(accum_count=1, alpha=0.95, batch_size=3000, beam_size=5, bert_data_path='/data/bqw/nlp_data/cnn_bert/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks='0', graph_drop=0.2, graph_hdim=768, graph_layers=3, init_method='tcp://localhost:10000', label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/val_abs_bert_cnndm', lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='../models/', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, rel_num=47, report_every=1, report_rouge=True, result_path='../logs/abs_bert_cnndm', save_checkpoint_steps=5, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=500, test_from='../models/abs_bert/model_step_2000.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_emb=False, use_interval=True, visible_gpus='0', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-task\", default='ext', type=str, choices=['ext', 'abs'])\n",
    "parser.add_argument(\"-encoder\", default='bert', type=str, choices=['bert', 'baseline'])\n",
    "parser.add_argument(\"-mode\", default='test', type=str, choices=['train', 'validate', 'test'])\n",
    "parser.add_argument(\"-bert_data_path\", default='../bert_data_new/cnndm')\n",
    "parser.add_argument(\"-model_path\", default='../models/')\n",
    "parser.add_argument(\"-result_path\", default='../results/cnndm')\n",
    "parser.add_argument(\"-temp_dir\", default='../temp')\n",
    "parser.add_argument('-init_method', default='tcp://localhost:10000')\n",
    "\n",
    "parser.add_argument(\"-batch_size\", default=140, type=int)\n",
    "parser.add_argument(\"-test_batch_size\", default=200, type=int)\n",
    "\n",
    "parser.add_argument(\"-max_pos\", default=512, type=int)\n",
    "parser.add_argument(\"-use_interval\", type=str2bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-large\", type=str2bool, nargs='?',const=True,default=False)\n",
    "parser.add_argument(\"-load_from_extractive\", default='', type=str)\n",
    "\n",
    "parser.add_argument(\"-sep_optim\", type=str2bool, nargs='?',const=True,default=False)\n",
    "parser.add_argument(\"-lr_bert\", default=2e-3, type=float)\n",
    "parser.add_argument(\"-lr_dec\", default=2e-3, type=float)\n",
    "parser.add_argument(\"-use_bert_emb\", type=str2bool, nargs='?',const=True,default=False)\n",
    "\n",
    "parser.add_argument(\"-share_emb\", type=str2bool, nargs='?', const=True, default=False)\n",
    "parser.add_argument(\"-finetune_bert\", type=str2bool, nargs='?', const=True, default=True)\n",
    "parser.add_argument(\"-dec_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-dec_layers\", default=6, type=int)\n",
    "parser.add_argument(\"-dec_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-dec_heads\", default=8, type=int)\n",
    "parser.add_argument(\"-dec_ff_size\", default=2048, type=int)\n",
    "parser.add_argument(\"-enc_hidden_size\", default=512, type=int)\n",
    "parser.add_argument(\"-enc_ff_size\", default=512, type=int)\n",
    "parser.add_argument(\"-enc_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-enc_layers\", default=6, type=int)\n",
    "\n",
    "# params for EXT\n",
    "parser.add_argument(\"-ext_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-ext_layers\", default=2, type=int)\n",
    "parser.add_argument(\"-ext_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-ext_heads\", default=8, type=int)\n",
    "parser.add_argument(\"-ext_ff_size\", default=2048, type=int)\n",
    "\n",
    "parser.add_argument(\"-label_smoothing\", default=0.1, type=float)\n",
    "parser.add_argument(\"-generator_shard_size\", default=32, type=int)\n",
    "parser.add_argument(\"-alpha\",  default=0.6, type=float)\n",
    "parser.add_argument(\"-beam_size\", default=5, type=int)\n",
    "parser.add_argument(\"-min_length\", default=15, type=int)\n",
    "parser.add_argument(\"-max_length\", default=150, type=int)\n",
    "parser.add_argument(\"-max_tgt_len\", default=140, type=int)\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"-param_init\", default=0, type=float)\n",
    "parser.add_argument(\"-param_init_glorot\", type=str2bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-optim\", default='adam', type=str)\n",
    "parser.add_argument(\"-lr\", default=1, type=float)\n",
    "parser.add_argument(\"-beta1\", default= 0.9, type=float)\n",
    "parser.add_argument(\"-beta2\", default=0.999, type=float)\n",
    "parser.add_argument(\"-warmup_steps\", default=8000, type=int)\n",
    "parser.add_argument(\"-warmup_steps_bert\", default=8000, type=int)\n",
    "parser.add_argument(\"-warmup_steps_dec\", default=8000, type=int)\n",
    "parser.add_argument(\"-max_grad_norm\", default=0, type=float)\n",
    "\n",
    "parser.add_argument(\"-save_checkpoint_steps\", default=5, type=int)\n",
    "parser.add_argument(\"-accum_count\", default=1, type=int)\n",
    "parser.add_argument(\"-report_every\", default=1, type=int)\n",
    "parser.add_argument(\"-train_steps\", default=1000, type=int)\n",
    "parser.add_argument(\"-recall_eval\", type=str2bool, nargs='?',const=True,default=False)\n",
    "\n",
    "\n",
    "parser.add_argument('-visible_gpus', default='-1', type=str)\n",
    "parser.add_argument('-gpu_ranks', default='0', type=str)\n",
    "parser.add_argument('-log_file', default='../logs/cnndm.log')\n",
    "parser.add_argument('-seed', default=666, type=int)\n",
    "\n",
    "parser.add_argument(\"-test_all\", type=str2bool, nargs='?',const=True,default=False)\n",
    "parser.add_argument(\"-test_from\", default='')\n",
    "parser.add_argument(\"-test_start_from\", default=-1, type=int)\n",
    "\n",
    "parser.add_argument(\"-train_from\", default='')\n",
    "parser.add_argument(\"-report_rouge\", type=str2bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-block_trigram\", type=str2bool, nargs='?', const=True, default=True)\n",
    "\n",
    "parser.add_argument('-graph_layers', default=3, type=int)\n",
    "parser.add_argument('-rel_num', default=46+1, type=int)\n",
    "parser.add_argument('-graph_hdim', default=768, type=int)\n",
    "parser.add_argument('-graph_drop', default=0.2, type=float)\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args('-task abs -mode test -batch_size 3000 -test_batch_size 500 -bert_data_path /data/bqw/nlp_data/cnn_bert/cnndm -log_file ../logs/val_abs_bert_cnndm -test_from ../models/abs_bert/model_step_2000.pt -sep_optim true -use_interval true -visible_gpus 0 -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 -result_path ../logs/abs_bert_cnndm'.split())\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_builder import AbsSummarizer\n",
    "model = AbsSummarizer(args, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AbsSummarizer(args, 0,checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a, b in zip(model.named_parameters(), list(checkpoint['model'].keys())):\n",
    "#     print(a[0],b)\n",
    "\n",
    "# for a in list(checkpoint['model'].keys()):\n",
    "#     print(a)\n",
    "#     if a=='decoder.transformer_layers.5.context_attn_src.linear_keys.weight':\n",
    "#         print(checkpoint['model']['decoder.transformer_layers.5.context_attn_src.linear_keys.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['model'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tile(x, count, dim=0):\n",
    "    \"\"\"\n",
    "    Tiles x on dimension dim count times.\n",
    "    \"\"\"\n",
    "    perm = list(range(len(x.size())))\n",
    "    if dim != 0:\n",
    "        perm[0], perm[dim] = perm[dim], perm[0]\n",
    "        x = x.permute(perm).contiguous()\n",
    "    out_size = list(x.size())\n",
    "    out_size[0] *= count\n",
    "    batch = x.size(0)\n",
    "    x = x.view(batch, -1) \\\n",
    "         .transpose(0, 1) \\\n",
    "         .repeat(count, 1) \\\n",
    "         .transpose(0, 1) \\\n",
    "         .contiguous() \\\n",
    "         .view(*out_size)\n",
    "    if dim != 0:\n",
    "        x = x.permute(perm).contiguous()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 26])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gents=torch.ones((6, 26, 768))\n",
    "emask = torch.arange(0, gents.size(1)).unsqueeze(0).repeat(gents.size(0), 1)\n",
    "# emask = (emask <= elens.unsqueeze(1)).to(self.get_device())\n",
    "emask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 26, 768])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile(gents, 2, 0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
